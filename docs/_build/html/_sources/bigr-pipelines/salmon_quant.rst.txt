.. _`salmon_quant`:

SALMON_QUANT
============

Perform trimming and quantification on RNASeq

Usage
-----

In order to run the pipeline, use the following commands

.. code-block:: bash 

  # Go to your working directory

  cd /path/to/my/working/directory

  # Build a design file (see below)

  # Copy/paste the following line for **HG19**

  bash /mnt/beegfs/pipelines/snakemake-wrappers/bigr_pipelines/salmon_quant/run.sh hg19

  # Copy/paste the following line for **HG38**

  bash /mnt/beegfs/pipelines/snakemake-wrappers/bigr_pipelines/salmon_quant/run.sh hg38


Input/Output
------------


**Input:**

 
  
* Fastq files
  
 
  
* Fasta-formatted Genome sequence
  
 
  
* Fasta-formatted transcriptome sequence
  
 
  
* GTF formatted genome annotation
  
 


**Output:**

 
  
* Salmon quantification (Raw.genes.tsv + TMP.genes.tsv + TPM.transcripts.tsv)
  
 
  
* MultiQC report handling Quality controls
  
 
  
* Trimmed fastq files
  
 







Notes
-----

Prerequisites:

* A TSV formatted design file, *named 'design.tsv'* with the following columns:

.. list-table:: Desgin file format
  :widths: 33 33 33
  :header-rows: 1

  * - Sample_id
    - Upstream_fastq
    - Downstream_fastq
  * - Name of the Sample1
    - Path to upstream fastq file
    - Path to downstream fastq file
  * - Name of the Sample2
    - Path to upstream fastq file
    - Path to downstream fastq file
  * - ...
    - ...
    - ...


Warning: If available, the salmon index passed in configuration will be used, wather you changed the command line arguments or not.


How does it work ?

1. Gathering Fastq files

This pipeline copies files from iRODS, or symlinks them. In your design file,
the `Sample_id` is used to rename your fastq file and make them all easily
recognisable. In case of iRODS copy, the checksum is automatically computed
and verified. In cas of a copy from cold to hot storage, then a checksum is
automatically computed and verified. Elsewise, a simple sylmink is done and
no control needs to be performed.

The column Upstream_file/Downstream_file identifies reads' streams.
If the sequencing was not oriented, then order does not matter.
Otherwise, make sure R1 reads are under Upstream_file, and R2 reads under
Downstream_file.

You may need to concatenate several fastq files into one single fastq file
for a given sample: in case of lane splitting, run splitting, and/or
resequencing. This may be done automatically! Under the corresponding column,
separate the multiple files by a comma (`,`).

2. Cleaning fastq files

These (concatenated?) fastq files are trimmed with fastp. By default, the
cleaning is a bit more relaxed than default fastp parameters. We're woring on
RNA-Seq with Salmon in this pipeline. No need to be very strict.

The running window has been increased to 6 nucleotides, the minimum mean read
quality was raised to 10. The unqualified percent limit was raised to 50% and
the maximum of N bases was raised to 7. The minimum length was lowered to 15.
Because we are trimming RNA-Seq, we analyse overrepresented sequences.

Nothing dramatic! Bad reads will be filtered later if needed.

By the way, FastQ Screen is used over the raw reads, since possible sequencing
artifacts are interesting in this step. Many genomes are tested, see section
5 to look at the results.

3. Genome indexation

In parallel of this trimming, a genome indexation is done for your analysis.
Taking your read length into account, we build an ideal index using decoy
sequences and best hash seed size for the selective alignment. Yes, you read
it, no more pseudo-mapping on transcriptome. We are doing selective alignment
over the decoy-aware gentrome! Welcome in 2019!

During this indexation step, duplicates are kept, since fellow biologists
always want all the targets to be tested.

4. Selective Alignment and Quantification

Salmon is used to run the selctive alignment. It modelise the possible GC
bias, initial sequence bias and positional bias, since we empirically always
see them on our quality controls.

Mappings are validated, and 100 bootstraps are done over the quantification.

5. Quality report

MultiQC is used to aggregate quality reports from Fastp and Salmon. Quite
handy, isn't it?





Snakefile
---------

The pipeline contains the following steps:

.. code-block:: python

    #####################################
    # Check Snakemake version           #
    # Import search-and-buil functions  #
    # for config and design             #
    #####################################

    from snakemake.utils import min_version
    from pathlib import Path
    from yaml import dump

    min_version("7.5")

    import sys

    worflow_source_dir = Path(snakemake.workflow.srcdir("."))
    common = str(worflow_source_dir / ".." / "common" / "python")
    sys.path.append(common)

    from file_manager import *
    from files_linker import *
    from write_yaml import *
    from messages import message

    #####################
    # Setup environment #
    #####################

    logging.basicConfig(
        filename="snakemake.salmon_quant.log", filemode="w", level=logging.DEBUG
    )


    default_config = read_yaml(worflow_source_dir / "config.hg38.yaml")
    configfile: get_config(default_config)
    design = get_design(os.getcwd(), search_fastq_pairs)

    ##################################
    # Setup globals and fix wilcards #
    ##################################

    fastq_links = link_fq(design.Sample_id, design.Upstream_file, design.Downstream_file)


    wildcard_constraints:
        sample=r"|".join(design.Sample_id.to_list()),
        stream=r"|".join(map(str, range(3))),


    #################################################
    ### Gather files from iRODS or mounting point ###
    #################################################


    include: "rules/001.bigr_copy.smk"


    ############################
    ### FASTP FASTQ CLEANING ###
    ############################


    # See quality controls & trimming:
    # at: rules/001.qc.smk


    ########################
    ### Quality Controls ###
    ########################


    include: "rules/002.qc.smk"



    #############################
    ### Salmon quantification ###
    #############################


    include: "rules/003.salmon.smk"

    ########################
    ### Aggregate counts ###
    ########################


    include: "rules/004.aggregate.smk"


    #################
    ### Main rule ###
    #################


    rule target_salmon_quant:
        input:
            "multiqc/MultiQC.html",
            "salmon/TPM.genes.tsv",
            "salmon/TPM.transcripts.tsv",
            "salmon/Raw.genes.tsv"
        output:
            directory("results_to_upload")
        threads: 1
        resources:
            mem_mb=lambda wildcards, attempt: attempt * 512,
            time_min=lambda wildcards, attempt: attempt * 25,
            tmpdir="tmp"
        log:
            "logs/results_to_upload.log"
        params:
            "--verbose --checksum --human-readable"
        shell:
            "rsync {params} {input} {output} > {log} 2>&1"




Authors
-------


* Thibault Dayris

* M boyba Diop

* Marc Deloger
