.. _`Variant_Calling_Somatic`:

VARIANT_CALLING_SOMATIC
=======================

Perform Variant calling on Somatic

Usage
-----

In order to run the pipeline, use the following commands

.. code-block:: bash 

  # Go to your working directory

  cd /path/to/my/working/directory

  # Build a design file (see below)

  # Copy/paste the following line for **HG19**

  bash /mnt/beegfs/pipelines/snakemake-wrappers/bigr_pipelines/variant_calling_somatic/run.sh hg19

  # Copy/paste the following line for **HG38**

  bash /mnt/beegfs/pipelines/snakemake-wrappers/bigr_pipelines/variant_calling_somatic/run.sh hg38

  # Copy/paste the following line for **MM10**

  bash /mnt/beegfs/pipelines/snakemake-wrappers/bigr_pipelines/variant_calling_somatic/run.sh mm10


Input/Output
------------


**Input:**

 
  
* Pairs of Tumor Fastq-formatted sequence files
  
 
  
* Pairs of corresponding Non-Tumor Fastq-formatted sequence files
  
 
  
* Cosmic database formatted as gzipped vcf and its tbi index (already provided for IGR Flamingo users)
  
 
  
* dbSNP database formatted as gzipped vcf and its tbi index (already provided for IGR Flamingo users)
  
 
  
* MSigDB database formatted as GMT (already provided for IGR Flamingo users)
  
 
  
* GWASCatalog database formatted as TSV (already provided for IGR Flamingo users)
  
 
  
* Kaviar database formatted as gzipped vcf and its tbi index (already provided for IGR Flamingo users)
  
 
  
* SnpEff database downloaded with SnpEff itself (already provided for IGR Flamingo users)
  
 
  
* CaptureKit genomic intervals formatted as BED (already provided for IGR Flamingo users)
  
 
  
* Known variants from dbSNP, with only AF within the INFO field for GATK (already provided for IGR Flamingo users)
  
 
  
* dbNSFP database formatted as TSV (already provided for IGR Flamingo users)
  
 
  
* FastQ Screen databases (already provided for IGR Flamingo users)
  
 


**Output:**

 
  
* Annotated VCF files
  
 
  
* MultiQC report
  
 





Used meta-wrappers
------------------

The following individual meta-wrappers are used in this pipeline:


* :ref:`meta/bio/bwa_fixmate`

* :ref:`meta/bio/gatk_bqsr`

* :ref:`meta/bio/varscan2_calling`

* :ref:`bigr_pipelines/snpeff_snpsift`


Please refer to each meta-wrapper in above list for additional configuration parameters and information about the executed code.




Used wrappers
-------------

The following individual wrappers are used in this pipeline:


* :ref:`bio/bigr/copy`

* :ref:`bio/fastp`

* :ref:`bio/compress/pbgzip`

* :ref:`bio/tabix`

* :ref:`bio/multiqc`

* :ref:`bio/picard/collectalignmentsummarymetrics`

* :ref:`bio/fastq_screen`


Please refer to each wrapper in above list for additional configuration parameters and information about the executed code.




Notes
-----

Prerequisites for piREST:

* A TSV formatted design file, *named 'design.tsv'* with the following columns:

.. list-table:: Desgin file format
  :widths: 33 33 33
  :header-rows: 1

  * - Sample_id
    - Upstream_fastq
    - Downstream_fastq
    - datasetIdBED
    - genomeVersion
  * - Name of the Sample1
    - Path to upstream fastq file
    - Path to downstream fastq file
    - dataset Id corresponding to the axpected bed file
    - the genome version used for this sample
  * - Name of the Sample2
    - Path to upstream fastq file
    - Path to downstream fastq file
    - dataset Id corresponding to the axpected bed file
    - the genome version used for this sample
  * - ...
    - ...
    - ...





Snakefile
---------

The pipeline contains the following steps:

.. code-block:: python

    import datetime
    import logging
    import os
    import pandas
    import sys
    from pathlib import Path

    worflow_source_dir = Path(next(iter(workflow.get_sources()))).absolute().parent
    common = str(worflow_source_dir / "../common/python")
    sys.path.append(common)

    from file_manager import *
    from files_linker import *
    from write_yaml import *
    from messages import *
    from snakemake.utils import min_version
    min_version("6.0")

    logging.basicConfig(
        filename="snakemake.variant_calling_somatic.log",
        filemode="w",
        level=logging.DEBUG
    )

    container: "docker://continuumio/miniconda3:4.4.10"
    localrules: bigr_copy

    ruleorder: sambamba_index_bam > sambamba_index
    ruleorder: gatk_filter_mutect_calls > tabix_index
    ruleorder: mutect2_somatic > tabix_index

    default_config = read_yaml(worflow_source_dir / "config.hg38.yaml")
    configfile: get_config(default_config)
    design = get_design(os.getcwd(), search_fastq_somatic)
    #design = design.head(2).tail(1)
    design.dropna(inplace=True)
    #print(design)

    design.index = design["Sample_id"]
    #design.drop(index="s070", inplace=True)

    wildcard_constraints:
        sample = r"|".join(design["Sample_id"]),
        stream = r"1|2|R1|R2",
        status = r"normal|tumor",
        content = r"snp|indel"


    fastq_links = link_fq_somatic(
        sample_names=design.Sample_id,
        n1_paths=design.Upstream_file_normal,
        t1_paths=design.Upstream_file_tumor,
        n2_paths=design.Downstream_file_normal,
        t2_paths=design.Downstream_file_tumor,
    )

    ruleorder: fix_annotation_for_gatk > pbgzip_compress
    ruleorder: gatk_variant_filtration > pbgzip_compress

    rule all:
        input:
            #maf="maf/complete.maf",
            #mafs=expand(
            #    "maf/maftools/{sample}.maf",
            #    sample=design["Sample_id"].tolist()
            #),
            #calls=expand(
            #   "maf/occurence_annotated/{sample}.vcf.gz{index}",
            #   sample=design["Sample_id"].tolist(),
            #   index=["", ".tbi"]
            # ),
            #mutect2=expand(
            #    "bcftools/mutect2/{sample}.vcf.gz",
            #    sample=design["Sample_id"].tolist()
            #),
            #mutect2_tbi=expand(
            #    "bcftools/mutect2/{sample}.vcf.gz.tbi",
            #    sample=design["Sample_id"].tolist()
            #),
            #annotated_vcf=expand(
            #    "snpeff_snpsift/results_to_upload",
            #),
            facets=expand(
                "facets/{sample}/{sample}.{ext}",
                sample=design["Sample_id"].tolist(),
                ext=["vcf.gz", "cnv.png", "cov.pdf", "spider.pdf", "csv.gz"]
            ),
            calls=expand(
                "snpeff_snpsift/snpsift/fixed/{sample}.vcf.gz",
                sample=design["Sample_id"]
            ),
            calls_index=expand(
                "snpeff_snpsift/snpsift/fixed/{sample}.vcf.gz.tbi",
                sample=design["Sample_id"]
            ),
            table=expand(
                "snpeff_snpsift/snpsift/extractFields/{sample}.tsv",
                sample=design["Sample_id"]
            ),
            html="snpeff_snpsift/multiqc/SnpEff_annotation.html",
            html_data="snpeff_snpsift/multiqc/SnpEff_annotation_data",
            snpeff=expand(
                "snpeff_snpsift/snpeff/csvstats/{sample}.{ext}",
                sample=design["Sample_id"],
                ext=["csv", "genes.txt"]
            ),
            #varscan2=expand(
            #    "bcftools/varscan2/{sample}.vcf.gz",
            #    sample=design["Sample_id"].tolist()
            #),
            #varscan2_tbi=expand(
            #    "bcftools/varscan2/{sample}.vcf.gz.tbi",
            #    sample=design["Sample_id"].tolist()
            #),
            msisensor=expand(
                "msisensor/{sample}/{sample}.msi",
                sample=design["Sample_id"].tolist()
            ),
            qc="multiqc/variant_calling_somatic.html",
            #calling_result="final.vcf.list"
            results=expand("pandas/filter/{how}/dp{dp}/{sample}.dp{dp}.tsv",
                how=["all", "census_only", "oncokb_only"] if config.get("ANMO", False) is True else ["all"],
                sample=design["Sample_id"].tolist(),
                dp=["10", "40", "60"]
            ),
            results_xl=expand("pandas/filter/{how}/dp{dp}/{sample}.dp{dp}.xlsx",
                how=["all", "census_only", "oncokb_only"] if config.get("ANMO", False) is True else ["all"],
                sample=design["Sample_id"].tolist(),
                dp=["10", "40", "60"]
            )
        output:
            directory("results_to_upload")
        message:
            "Finishing the WES Somatic Variant Calling"
        threads: 1
        resources:
            mem_mb=lambda wildcards, attempt: attempt * 768,
            time_min=lambda wildcards, attempt: attempt * 45,
            tmpdir="tmp"
        log:
            "logs/resultdir.log"
        params:
            mk="--parents --verbose",
            rs="--checksum --verbose --human-readable",
            dirlist=" ".join([f"results_to_upload/{content}" for content in ["msi", "tmb", "vcf", "tsv", "xlsx", "cnv", "qc"]])
        shell:
            "mkdir {params.mk} {params.dirlist} > {log} 2>&1 && "
            "rsync {params.rs} {input.msisensor} {output}/msi/ >> {log} 2>&1 && "
            "rsync {params.rs} {input.results} {output}/tsv/ >> {log} 2>&1 && "
            "rsync {params.rs} {input.results} {output}/xlsx/ >> {log} 2>&1 && "
            "rsync {params.rs} {input.facets} {output}/cnv/ >> {log} 2>&1 && "
            "rsync {params.rs} {input.calls} {input.calls_index} {output}/vcf/ >> {log} 2>&1 && "
            "rsync {params.rs} {input.html} {input.qc} {output}/qc/ >> {log} 2>&1 "


    #########################
    ### ANMO-like filters ###
    #########################


    rule pandas_filter_tsv:
       input:
          table="snpeff_snpsift/snpsift/extractFields/{sample}.tsv"
       output:
          table="pandas/filter/{how}/dp{dp}/{sample}.dp{dp}.tsv",
          xlsx="pandas/filter/{how}/dp{dp}/{sample}.dp{dp}.xlsx"
       threads: 1
       resources:
          mem_mb=lambda wildcards, attempt: attempt * 1024 * 8,
          time_min=lambda wildcards, attempt: attempt * 15,
          tmpdir="tmp"
       log:
          "logs/pandas_filter/{sample}/{how}.{dp}.log"
       params:
          new_cols = lambda wildcards: [
             ["Mutect2_Allele_Frequency", "=", f"{wildcards.sample}_tumor_AF"]
          ],
          prefixes = [
             ["Chromosome", "chr"]
          ],
          keep_column = lambda wildcards: [
                "Chromosome",
                "Start_Position",
                "Variant_ID",
                "SYMBOL",
                "Reference_Allele",
                "Tumor_Seq_Allele1",
                "Mutect2_Read_depth",
                "Tumor_Seq_Allele2",
                "HGVSc",
                "HGVSp",
                "VarOcc",
                "BIOTYPE",
                "IMPACT",
                "Variant_Classification",
                "Mutect2_Read_depth",
                "Mutect2_Allele_Frequency",
                "Variant_Type",
                "Tumor_Sample_Barcode",
                "dbSNP_Pubmed",
                "dbSNP_Clinical_Diagnostic_Assay",
                "dbNSFP_ExAC_AlleleFrequency",
                "Kaviar_Allele_Frequency",
                "MSigDb_Pathways",
                "dbNSFP_Polyphen2_HDIV_pred",
                "dbNSFP_SIFT_pred",
                "dbNSFP_FATHMM_pred",
                "dbNSFP_ClinPred_score",
                "dbNSFP_MutationTaster_pred",
                'dbNSFP_MutationAssessor_pred',
                "dbNSFP_MetaLR_pred",
                "dbNSFP_LIST_S2_pred",
                "dbNSFP_LRT_pred",
                "dbNSFP_BayesDel_noAF_pred",
                "dbNSFP_Aloft_pred",
                "Transcript_ID",
                "Gene",
                "Feature",
                "Filter",
                "Center",
                "Hugo_Symbol",
                "End_Position",
                "CancerGeneCensus_Gene_Symbol",
                "CancerGeneCensus_Name",
                "CancerGeneCensus_Entrez_GeneId",
                "CancerGeneCensus_Tier",
                "CancerGeneCensus_Somatic",
                "CancerGeneCensus_Germline",
                "CancerGeneCensus_Tumour_TypesSomatic",
                "OncoKB_Hugo_Symbol",
                "OncoKB_Entrez_Gene_ID",
                "OncoKB_GRCh37_Isoform",
                "OncoKB_GRCh37_RefSeq",
                "OncoKB_GRCh38_RefSeq",
                "OncoKB_OncoKB_Annotated",
                "OncoKB_Is_Oncogene",
                "OncoKB_Is_Tumor_Suppressor_Gene",
                "OncoKB_MSK_IMPACT",
                "OncoKB_MSK_HEME",
                f"{wildcards.sample}_normal_Reference_Allele",
                f"{wildcards.sample}_normal_Seq_Allele1",
                f"{wildcards.sample}_normal_Seq_Allele2",
                f"{wildcards.sample}_normal_DP",
                f"{wildcards.sample}_normal_AD_allele2",
                f"{wildcards.sample}_normal_AF",
                f"{wildcards.sample}_tumor_Reference_Allele",
                f"{wildcards.sample}_tumor_Seq_Allele1",
                f"{wildcards.sample}_tumor_Seq_Allele2",
                f"{wildcards.sample}_tumor_DP",
                f"{wildcards.sample}_tumor_AD_allele1",
                f"{wildcards.sample}_tumor_AD_allele2",
                f"{wildcards.sample}_tumor_AF",
          ],
          convert_cols_type = lambda wildcards: {
                "Mutect2_Allele_Frequency": "float",
                "Mutect2_Read_depth": "int",
                f"{wildcards.sample}_tumor_AF": "float",
                f"{wildcards.sample}_normal_AF": "float",
                #f"{wildcards.sample}_normal_AD_allele2": "float",
                f"{wildcards.sample}_tumor_DP": "int",
                #f"{wildcards.sample}_tumor_AD_allele2": "float",
          },
          filters = lambda wildcards: [
                # ["Variant_Classification", "!=", "downstream_gene_variant"],
                # ["Variant_Classification", "!=", "intergenic_region"],
                # ["Variant_Classification", "!=", "synonymous_variant"],
                # ["Variant_Classification", "!=", "non_coding_transcript_exon_variant"],
                # ["Variant_Classification", "!=", "upstream_gene_variant"],
                # ["Variant_Classification", "!=", "splice_region_variant&synonymous_variant"],
                # ["Variant_Classification", "!=", "non_coding_transcript_variant"],
                # ["Variant_Classification", "!=", "intron_variant"],
                ["Variant_Classification", "!=", "Synonymous_Variant"],
                #["Mutect2_Read_depth", ">=", int(wildcards.filter)],
                #['Mutect2_Allele_Frequency', ">=", 0.1],
                [f"{wildcards.sample}_tumor_AF", ">=", 0.1],
                [f"{wildcards.sample}_normal_AF", "<=", 0.1],
                #[f"{wildcards.sample}_normal_AD_allele2", "<=", float(wildcards.dp)],
                #[f"{wildcards.sample}_tumor_AD_allele2", ">=", float(wildcards.dp)],
                [f"{wildcards.sample}_tumor_DP", ">=", float(wildcards.dp)],
                #["dbNSFP_ExAC_AlleleFrequency", "<=", 0.05],
                ["VarOcc", "<=", len(design["Sample_id"].tolist()) - 1]
          ] if config.get("ANMO", False) is True else [],
          not_contains = lambda wildcards: [
                ["Filter", "germline"],
                ["Filter", "multiallelic"],
                #"Filter": "fragment",
                #"Filter": "contamination",
                #"Filter": "weak_evidence",
                #"Filter": "slippage",
                #"Filter": "strand_bias",
                #"Filter": "map_qual",
                ["Filter", "haplotype"],
                #"Filter": "base_qual",
                #["Filter", "DPBelow5"],
                ["Filter", "AboveFisherStrandBias"],
                ["Filter", "AboveStrandOddsRatio"],
                ["Filter", "BelowMQRankSum"],
                ["Filter", "BelowReadPosRankSum"],
                #["Filter", "VarOccAbove105"],
                #["Filter", f"DepthBelow{wildcards.filter}X"],
                #["Filter", "AFBelow40pct"]
          ],
          contains = lambda wildcards: [
                ["Filter", (
                   "ExistsInCancerGeneCensus" if wildcards.how == "census_only" else (
                      "ExistsInOncoKB" if wildcards.how == "oncokb_only" else ""))]
          ],
          drop_duplicated_lines=True
       wrapper:
            "bio/pandas/filter_table"


    ##################
    ### VCF to MAF ###
    ##################

    vcf_post_process_config = {
        "ncbi_build": config["params"].get("ncbi_build", "GRCh38"),
        "center": config["params"].get("center", "GustaveRoussy"),
        "annotation_tag": "ANN=",
        "sample_list": design["Sample_id"].to_list(),
        "genome": config["ref"]["fasta"],
        "known": config["ref"]["dbsnp"],
        "chr": config["params"]["chr"]
    }

    module vcf_post_process:
        snakefile: "../../meta/bio/vcf_post_process/test/Snakefile"
        config: vcf_post_process_config

    use rule * from vcf_post_process


    #################
    ### Gather QC ###
    #################

    rule multiqc:
        input:
            html=expand(
                "fastp/html/pe/{sample}_{status}.fastp.html",
                sample=design["Sample_id"],
                status=["normal", "tumor"]
            ),
            json=expand(
                "fastp/json/pe/{sample}_{status}.fastp.json",
                sample=design["Sample_id"],
                status=["normal", "tumor"]
            ),
            sambamba_metrics=expand(
                "sambamba/markdup/{sample}_{status}.bam",
                sample=design["Sample_id"],
                status=["normal", "tumor"]
            ),
            fastq_screen=expand(
                "fastq_screen/{sample}.{stream}.{status}.fastq_screen.{ext}",
                sample=design["Sample_id"],
                stream=["1", "2"],
                ext=["txt", "png"],
                status=["normal", "tumor"]
            ),
            picard_summary=expand(
                "picard/alignment_summary/{sample}_{status}.summary.txt",
                sample=design["Sample_id"],
                status=["normal", "tumor"]
            ),
            snpeff=expand(
                "snpeff_snpsift/csvstats/{sample}.genes.txt",
                sample=design["Sample_id"]
            )
        output:
            report(
                "multiqc/variant_calling_somatic.html",
                caption="../common/reports/multiqc.rst",
                category="Quality Controls"
            )
        message:
            "Aggregating quality reports from SnpEff"
        threads: 1
        resources:
            mem_mb=lambda wildcards, attempt: min(attempt * 1536, 10240),
            time_min=lambda wildcards, attempt: attempt * 35,
            tmpdir="tmp"
        log:
            "logs/multiqc.log"
        wrapper:
            "bio/multiqc"


    rule alignment_summary:
        input:
            bam="sambamba/sort/{sample}_{status}.bam",
            bam_index=get_bai("sambamba/sort/{sample}_{status}.bam"),
            ref=config['ref']['fasta'],
            ref_idx=get_fai(config['ref']['fasta']),
            ref_dict=get_dict(config['ref']['fasta']),
        output:
            temp("picard/alignment_summary/{sample}_{status}.summary.txt")
        message:
            "Collecting alignment metrics on GATK recalibrated {wildcards.sample}"
            " (considering {wildcards.status})"
        threads: 1
        resources:
            mem_mb=lambda wildcards, attempt: attempt * 1020,
            time_min=lambda wildcards, attempt: attempt * 45,
            tmpdir="tmp"
        log:
            "logs/picard/alignment_summary/{sample}_{status}.log"
        params:
            "VALIDATION_STRINGENCY=LENIENT "
            "METRIC_ACCUMULATION_LEVEL=null "
            "METRIC_ACCUMULATION_LEVEL=SAMPLE"
        wrapper:
            "bio/picard/collectalignmentsummarymetrics"


    rule fastq_screen:
        input:
            "reads/{status}/{sample}.{stream}.fq.gz"
        output:
            txt=temp("fastq_screen/{sample}.{stream}.{status}.fastq_screen.txt"),
            png=temp("fastq_screen/{sample}.{stream}.{status}.fastq_screen.png")
        message:
            "Assessing quality of {wildcards.sample}, {wildcards.stream}"
            " (considering {wildcards.status})"
        threads: config.get("threads", 20)
        resources:
            mem_mb=lambda wildcard, attempt: min(attempt * 1024 * 8, 20480),
            time_min=lambda wildcard, attempt: attempt * 75,
            tmpdir="tmp"
        params:
            fastq_screen_config=config["fastq_screen"],
            subset=100000,
            aligner='bowtie2'
        log:
            "logs/fastqc/{sample}.{stream}.{status}.log"
        wrapper:
            "bio/fastq_screen"


    ######################
    ### MSI sensor pro ###
    ######################

    msi_sensor_config = {
        "fasta": config["ref"]["fasta"],
        "bed": config["ref"]["capture_kit_bed"],
        "msi_scan_extra": config["msisensor_pro"].get("scan", ""),
        "msi_pro_extra": config["msisensor_pro"].get("msi", "")
    }

    module missensor_pro_meta:
        snakefile: "../../meta/bio/msi_sensor_pro/test/Snakefile"
        config: msi_sensor_config

    use rule * from missensor_pro_meta

    ##################
    ### CNV Facets ###
    ##################


    rule add_chr_to_pileup:
        input:
            "samtools/mpileup/{sample}.mpileup.gz"
        output:
            temp("samtools/mpileup/{sample}.chr.mpileup.gz")
        message:
            "Adding 'chr' to sequences on {wildcards.sample}"
        threads: 3
        resources:
            mem_mb=lambda wildcards, attempt: attempt * 512,
            time_min=lambda wildcards, attempt: attempt * 75,
            tmpdir="tmp"
        log:
            "logs/cnv_facets/add_chr/{sample}.log"
        params:
            gzip = "-c",
            awk = 'BEGIN {FS="\\t"; print "Chromosome\\tPosition\\tRef\\tAlt\\tFile1R\\tFile1A\\tFile1E\\tFile1D\\tFile2R\\tFile2A\\tFile2E\\tFile2D"} {if ($1 !~ /^chr*/) {print "chr"$0} else {print $0}}'
        shell:
            "gunzip {params.gzip} {input} | "
            "awk '{params.awk}' | "
            "gzip {params.gzip} > {output} 2> {log}"


    rule cnv_facets:
        input:
            #pileup="samtools/mpileup/{sample}.chr.mpileup.gz",
            tumor_bam="sambamba/markdup/{sample}_tumor.bam",
            normal_bam="sambamba/markdup/{sample}_normal.bam",
            vcf=config["ref"]["dbsnp"],
            vcf_index=get_tbi(config["ref"]["dbsnp"]),
            bed=config["ref"]["capture_kit_bed"]
        output:
            vcf="facets/{sample}/{sample}.vcf.gz",
            profile="facets/{sample}/{sample}.cnv.png",
            coverage="facets/{sample}/{sample}.cov.pdf",
            spider="facets/{sample}/{sample}.spider.pdf",
            pileup="facets/{sample}/{sample}.csv.gz"
        message:
            "Searching for CNV in {wildcards.sample} with Facets"
        threads: 10
        resources:
            mem_mb=lambda wildcards, attempt: (attempt * 1024 * 20) + 102400,
            time_min=lambda wildcards, attempt: attempt * 60 * 2,
            tmpdir="tmp"
        params:
            extra=config.get(
                "facets_extra",
                "--snp-count-orphans --gbuild hg38 --nbhd-snp 250"
            ),
            prefix="facets/{sample}/{sample}"
        log:
            "logs/facets/cnv/{sample}.log"
        wrapper:
            "bio/facets/cnv"


    #################################
    ### FINAL VCF FILE INDEXATION ###
    #################################

    module compress_index_vcf_meta:
        snakefile: "../../meta/bio/compress_index_vcf/test/Snakefile"
        config: config

    use rule * from compress_index_vcf_meta


    use rule tabix_index from compress_index_vcf_meta as snp_indel_tabix_index with:
        input:
            "{tool}/{subcommand}/{sample}.{content}.vcf.gz"
        output:
            "{tool}/{subcommand}/{sample}.{content}.vcf.gz.tbi"
        message:
            "Indexing {wildcards.sample} (from somatic varscan "
            "{wildcards.content}) with tabix."
        log:
            "logs/{tool}/{subcommand}/tabix/index/{sample}.{content}.log"


    use rule pbgzip_compress from compress_index_vcf_meta as si_pbgzip with:
        input:
            "{tool}/{subcommand}/{sample}.{content}.vcf"
        output:
            "{tool}/{subcommand}/{sample}.{content}.vcf.gz"
        message:
            "Compressnig {wildcards.sample} (from somatic varscan "
            "{wildcards.content}) with pbgzip."
        log:
            "logs/{tool}/{subcommand}/pgbzip/varcsanc2/{sample}.{content}.log"


    ######################
    ### VCF annotation ###
    ######################

    rule annotate_vcf:
        input:
            design="design.tsv",
            config="config.yaml",
            calls=expand(
                "mutect2/corrected/{sample}.vcf.gz",
                sample=design["Sample_id"]
            ),
            calls_index=expand(
                get_tbi("mutect2/corrected/{sample}.vcf.gz"),
                sample=design["Sample_id"]
            ),
        output:
            calls=temp(expand(
                "snpeff_snpsift/results_to_upload/VCF/{sample}.vcf.gz",
                sample=design["Sample_id"]
            )),
            calls_index=temp(expand(
                "snpeff_snpsift/results_to_upload/VCF/{sample}.vcf.gz.tbi",
                sample=design["Sample_id"]
            )),
            table=temp(expand(
                "snpeff_snpsift/results_to_upload/TSV/{sample}.tsv",
                sample=design["Sample_id"]
            )),
            html="snpeff_snpsift/results_to_upload/QC/SnpEff_annotation.html",
            raw_html=temp("snpeff_snpsift/multiqc/SnpEff_annotation.html"),
            raw_html_data=temp(directory("snpeff_snpsift/multiqc/SnpEff_annotation_data")),
            bigr_format_to_info=temp(expand(
                "snpeff_snpsift/bigr/format_to_info/{sample}.vcf",
                sample=design["Sample_id"]
            )),
            bigr_occurence_per_chr=temp(expand(
                "snpeff_snpsift/bigr/occurence/{chrom}.txt",
                chrom=config["params"]["chr"]
            )),
            bigr_occurence_annot=temp(expand(
                "snpeff_snpsift/bigr/occurence_annotated/{sample}.vcf",
                sample=design["Sample_id"]
            )),
            bigr_occurences=temp("snpeff_snpsift/bigr/occurences/all_chrom.txt"),
            config=temp("snpeff_snpsift/config.yaml"),
            column_description=temp("snpeff_snpsift/columns_description.txt"),
            design=temp("snpeff_snpsift/design.tsv"),
            logs=temp(directory("logs")),
            db_descriptions=temp(expand(
                "snpeff_snpsift/{db}/description.txt",
                db=["revel", "mane", "mistic"]
            )),
            snpsift_intermediar=temp(expand(
                "snpeff_snpsift/snpsift/{db}/{sample}.vcf",
                db=["clinvar", "dbnsfp", "dbsnp", "gmt", "kaviar", "vartype"],
                sample=design["Sample_id"]
            )),
            splice_ai=temp(expand(
                "snpeff_snpsift/splice_ai/annot/{sample}.vcf.gz",
                sample=design["Sample_id"]
            )),
            tmpdir=temp(directory("snpeff_snpsift/tmp")),
            vcftools=temp(expand(
                "snpeff_snpsift/vcftools/{db}/{sample}.vcf.gz",
                db=["revel", "mane", "mistic"],
                sample=design["Sample_id"]
            )),
            snpeff_raw_call=temp(expand(
                "snpeff_snpsift/snpeff/calls/{sample}.vcf{ext}",
                sample=design["Sample_id"],
                ext=["", ".gz", ".gz.tbi"]
            )),
            snpeff_raw_html=temp(expand(
                "snpeff_snpsift/snpeff/html/{sample}.html",
                sample=design["Sample_id"],
            )),
            snpeff_raw_csvstats=temp(expand(
                "snpeff_snpsift/snpeff/csvstats/{sample}.{ext}",
                sample=design["Sample_id"],
                ext=["csv", "genes.txt"]
            )),
            snpsift_raw_fields=temp(expand(
                "snpeff_snpsift/snpsift/extractFields/{sample}.tsv",
                sample=design["Sample_id"]
            )),
            snpsift_raw_vcf=temp(expand(
                "snpeff_snpsift/snpsift/fixed/{sample}.{ext}",
                sample=design["Sample_id"],
                ext=["vcf", "vcf.gz", "vcf.gz.tbi"]
            ))
            #results=directory("snpeff_snpsift/results_to_upload")
        message:
            "Annotating VCF"
        threads: 2
        resources:
            mem_mb=lambda wildcards, attempt: attempt * 1024 * 5,
            time_min=lambda wildcards, attempt: attempt * 60 * 4,
            tmpdir="tmp"
        handover: True
        log:
            "logs/snpeff_snpsift_pipeline.log"
        params:
            mkdir="--parents --verbose",
            ln="--symbolic --force --relative --verbose",
            variant_dir="mutect2/corrected/",
            outdir="snpeff_snpsift",
            pipeline_path=config.get(
                "snpeff_snpsift_run_path",
                "/mnt/beegfs/pipelines/snakemake-wrappers/bigr_pipelines/snpeff_snpsift/run.sh"
            ),
            smk="--nt",
            organism = config["params"].get("organism", "hg38")
        shell:
            "mkdir {params.mkdir} {params.outdir}/data_input > {log} 2>&1 && "
            "ln {params.ln} {input.config} {params.outdir} >> {log} 2>&1 && "
            "ln {params.ln} {params.variant_dir}/ {params.outdir}/data_input/calls >> {log} 2>&1 && "
            "cd {params.outdir} && "
            "bash {params.pipeline_path} {params.organism} {params.smk} | tee -a ${{OLDPWD}}/{log} 2>&1"


    #####################################
    ### Merge variant calling results ###
    #####################################

    # module metacaller_somatic_meta:
    #     snakefile: "../../meta/bio/meta_caller_somatic/test/Snakefile"
    #     config: {"genome": config["ref"]["fasta"], "bed": config["ref"]["capture_kit_bed"]}
    #
    #
    # use rule * from metacaller_somatic_meta as *


    ############################################################################
    ### Correcting Mutect2 :                                                 ###
    ### AS_FilterStatus: Number=1 and not Number=A which violates VCF format ###
    ############################################################################

    rule correct_mutect2_vcf:
        input:
            "bcftools/mutect2/{sample}.vcf.gz"
        output:
            temp("mutect2/corrected/{sample}.vcf")
        message:
            "Fixing AS_FilterStrand format error"
            " on {wildcards.sample}"
        threads: 2
        resources:
            mem_mb=lambda wildcards, attempt: attempt * 256,
            time_min=lambda wildcards, attempt: attempt * 20,
            tmpdir="tmp"
        log:
            "logs/mutect2/correct_fields/{sample}.log"
        params:
            fix_as_filterstatus="'s/ID=AS_FilterStatus,Number=A/ID=AS_FilterStatus,Number=1/g'"
        shell:
            "(gunzip -c {input} | "
            "sed {params.fix_as_filterstatus}) "
            "> {output} 2> {log}"

    ###############################
    ### Variant calling Mutect2 ###
    ###############################

    gatk_mutect2_somatic_config = {
        "genome": config["ref"]["fasta"],
        "known": config["ref"]["af_only"],
        "bed": config["ref"]["capture_kit_bed"],
        "dbsnp": config["ref"]["dbsnp"],
        "sample_list": design["Sample_id"].to_list(),
        "chrom": config["params"]["chr"]
    }


    module gatk_mutect2_somatic_meta:
        snakefile: "../../meta/bio/mutect2_somatic/test/Snakefile"
        config: gatk_mutect2_somatic_config


    use rule * from gatk_mutect2_somatic_meta


    ################################
    ### Variant Calling Varscan2 ###
    ################################

    varscan2_somatic_config = {
        "genome": config["ref"]["fasta"],
        "bed": config["ref"]["capture_kit_bed"]
    }

    module varscan2_somatic_meta:
        snakefile: "../../meta/bio/varscan2_somatic/test/Snakefile"
        config: varscan2_somatic_config

    use rule * from varscan2_somatic_meta


    ##############################
    ### GATK BAM RECALIBRATION ###
    ##############################

    gatk_bqsr_config = {
        "threads": config["threads"],
        "genome": config["ref"]["fasta"],
        "dbsnp": config["ref"]["dbsnp"],
        "base_recal_extra": "",
        "apply_base_recal_extra": config.get(
            "gatk", {"apply_base_recal_extra": "--create-output-bam-index"}
        ).get("apply_base_recal_extra", "--create-output-bam-index")
    }

    module gatk_bqsr_meta:
        snakefile: "../../meta/bio/gatk_bqsr/test/Snakefile"
        config: gatk_bqsr_config


    use rule gatk_apply_baserecalibrator from gatk_bqsr_meta with:
        input:
            bam="sambamba/markdup/{sample}_{status}.bam",
            bam_index=get_bai("sambamba/markdup/{sample}_{status}.bam"),
            ref=config['ref']['fasta'],
            ref_idx=get_fai(config['ref']['fasta']),
            ref_dict=get_dict(config['ref']['fasta']),
            recal_table="gatk/recal_data_table/{sample}_{status}.grp"
        output:
            bam="gatk/recal_bam/{sample}_{status}.bam",
            bai=get_bai("gatk/recal_bam/{sample}_{status}.bam")
        message:
            "Applying BQSR on {wildcards.status} {wildcards.sample} with GATK"
        params:
            extra=config.get(
                "gatk", {"apply_base_recal_extra", "--create-output-bam-index"}
            ).get("apply_base_recal_extra", "--create-output-bam-index")
        log:
            "logs/gatk/applybqsr/{sample}.{status}.log"


    use rule gatk_compute_baserecalibration_table from gatk_bqsr_meta with:
        input:
            bam="sambamba/markdup/{sample}_{status}.bam",
            bam_index=get_bai("sambamba/markdup/{sample}_{status}.bam"),
            ref=config['ref']['fasta'],
            ref_idx=get_fai(config['ref']['fasta']),
            ref_dict=get_dict(config['ref']['fasta']),
            known=config['ref']['dbsnp'],
            known_idx=get_tbi(config['ref']['dbsnp'])
        output:
            recal_table=temp("gatk/recal_data_table/{sample}_{status}.grp")
        message:
            "Compute BQSR table from {wildcards.status} {wildcards.sample} "
            "with GATK"
        log:
            "logs/gatk3/compute_bqsr/{sample}.{status}.log"


    #####################
    ### Deduplicating ###
    #####################

    rule sambamba_markduplicates:
        input:
            bam="samtools/filter/{sample}_{status}.bam",
            bai=get_bai("samtools/filter/{sample}_{status}.bam")
        output:
            bam=temp("sambamba/markdup/{sample}_{status}.bam")
        message:
            "Removing duplicates on {wildcards.sample} ({wildcards.status})"
        threads: 10
        resources:
            mem_mb=lambda wildcards, attempt: attempt * 10240,
            time_min=lambda wildcards, attempt: attempt * 45,
            tmpdir="tmp"
        log:
            "logs/sambamba/markduplicates/{sample}_{status}.log"
        params:
            extra = config.get(
                "sambamba", {"markdup": "--remove-duplicates"}
            ).get("markdup", "--remove-duplicates")
        wrapper:
            "bio/sambamba/markdup"



    """
    Filter a bam over the capturekit bed file
    """
    rule samtools_filter_bed:
        input:
            "sambamba/sort/{sample}_{status}.bam"
            fasta=config["genome"],
            fasta_idx=get_fai(config["genome"]),
            fasta_dict=get_dict(config["genome"]),
            bed=config["bed"]
        output:
            temp("samtools/filter/{sample}_{status}.bam")
        threads: 10
        resources:
            mem_mb=lambda wildcards, attempt: attempt * 2048,
            time_min=lambda wildcards, attempt: attempt * 15,
            tmpdir="tmp"
        params:
            extra="--bam --with-header"
        log:
            "logs/samtools/filter/{sample}_{status}.log"
        wrapper:
            "bio/samtools/view"


    ###################
    ### BWA MAPPING ###
    ###################

    module bwa_fixmate_meta:
        snakefile: "../../meta/bio/bwa_fixmate/test/Snakefile"
        config: {"threads": config["threads"], "genome": config["ref"]["fasta"]}


    def get_best_bwa_index():
        """Return cached data if available"""
        if config["ref"]["fasta"] == "/mnt/beegfs/database/bioinfo/Index_DB/Fasta/Ensembl/GRCh38.99/GRCh38.99.homo_sapiens.dna.main_chr.fasta":
            return multiext(
                "bwa_mem2/index/genome.hg38", ".0123", ".amb", ".ann", ".pac"
            )
        elif config["ref"]["fasta"] == "/mnt/beegfs/database/bioinfo/Index_DB/Fasta/Ensembl/GRCh37.75/Homo_sapiens.GRCh37.75.dna.primary_assembly.fa":
            return multiext(
                "bwa_mem2/index/genome.hg19", ".0123", ".amb", ".ann", ".pac"
            )
        elif config["ref"]["fasta"] == "/mnt/beegfs/database/bioinfo/Index_DB/Fasta/Ensembl/GRCm38.99/GRCm38.99.mus_musculus.dna.fasta":
            return multiext(
                "bwa_mem2/index/genome.mm10", ".0123", ".amb", ".ann", ".pac"
            )
        return multiext(
            "bwa_mem2/index/genome", ".0123", ".amb", ".ann", ".pac"
        )


    use rule sambamba_index from bwa_fixmate_meta with:
        input:
            "sambamba/sort/{sample}_{status}.bam"
        output:
            temp("sambamba/sort/{sample}_{status}.bam.bai")
        message:
            "Indexing mapped reads of {wildcards.status} {wildcards.sample}"
        log:
            "logs/sambamba/sort/{sample}.{status}.log"


    use rule sambamba_sort_coordinate from bwa_fixmate_meta with:
        input:
            mapping="samtools/fixmate/{sample}_{status}.bam"
        output:
            mapping=temp("sambamba/sort/{sample}_{status}.bam")
        message:
            "Sorting {wildcards.status} {wildcards.sample} reads by position"
        log:
            "logs/sambamba/sort/{sample}.{status}.log"


    use rule samtools_fixmate from bwa_fixmate_meta with:
        input:
            "bwa_mem2/mem/{sample}_{status}.bam"
        output:
            temp("samtools/fixmate/{sample}_{status}.bam")
        message:
            "Fixing mate annotation on {wildcards.status} "
            "{wildcards.sample} with Samtools"
        log:
            "logs/samtools/fixmate/{sample}.{status}.log"


    use rule bwa_mem from bwa_fixmate_meta with:
        input:
            reads=expand(
                "fastp/trimmed/pe/{sample}_{status}.{stream}.fastq",
                stream=["1", "2"],
                allow_missing=True
            ),
            index=get_best_bwa_index()
        output:
            temp("bwa_mem2/mem/{sample}_{status}.bam")
        message:
            "Mapping {wildcards.status} {wildcards.sample} with BWA"
        params:
            index=lambda wildcards, input: os.path.splitext(input["index"][0])[0],
            extra="-R '@RG\tID:{sample}_{status}\tSM:{sample}_{status}\tPU:{sample}_{status}\tPL:ILLUMINA\tCN:IGR\tDS:WES\tPG:BWA-MEM2' -M -A 2 -E 1",
            sort="samtools",         # We chose Samtools to sort by queryname
            sort_order="queryname",  # Queryname sort is needed for a fixmate
            sort_extra="-m 1536M"     # We extand the sort buffer memory
        log:
            "logs/bwa_mem2/mem/{sample}.{status}.log"

    # rule bwa_samblaster_sambamba:
    #     input:
    #         reads=expand(
    #             "fastp/trimmed/pe/{sample}_{status}.{stream}.fastq",
    #             stream=["1", "2"],
    #             allow_missing=True
    #         ),
    #         index=get_best_bwa_index()
    #     output:
    #         bam=temp("bwa_mem2/mem/{sample}_{status}.bam")
    #     message:
    #         "Mapping {wildcards.status} {wildcards.sample} with BWA, removing "
    #         "duplicates with Samblaster, sorting and filtering with Sambamba."
    #     threads:
    #         min(config.get("threads", 20), 20)
    #     threads: 1
    #     resources:
    #         mem_mb=lambda wildcards, attempt: attempt * 6144 + 61440,
    #         time_min=lambda wildcards, attempt: attempt * 120,
    #         tmpdir="tmp"
    #     shadow: "shallow"
    #     params:
    #         index=get_best_bwa_index()[0].split('.')[0],
    #         extra=lambda wildcards: f"-R '@RG\tID:{wildcards.sample}_{wildcards.status}\tSM:{wildcards.sample}_{wildcards.status}\tPU:{wildcards.sample}_{wildcards.status}\tPL:ILLUMINA\tCN:IGR\tDS:WES\tPG:BWA-MEM2' -M -A 2 -E 1",
    #         sort_extra=config.get("sambamba", {"sort_extra": ""}).get("sort_extra", ""),
    #         samblaster_extra=config.get("samblaster", {"extra": ""}).get("extra", ""),
    #         sambamba_view_extra=config.get("sambamba", {"view_extra": "-h"}).get("view_extra", "-h")
    #     log:
    #         "logs/bwa_samblaster_sambamba/{sample}_{status}.log"
    #




    use rule bwa_index from bwa_fixmate_meta with:
        input:
            config["ref"]["fasta"]


    use rule bwa_index_hg38 from bwa_fixmate_meta

    use rule bwa_index_hg19 from bwa_fixmate_meta

    use rule bwa_index_mm10 from bwa_fixmate_meta


    ############################
    ### FASTP FASTQ CLEANING ###
    ############################

    rule fastp_clean:
        input:
            sample=expand(
                "reads/{status}/{sample}.{stream}.fq.gz",
                stream=["1", "2"],
                allow_missing=True
            ),
        output:
            trimmed=temp(expand(
                "fastp/trimmed/pe/{sample}_{status}.{stream}.fastq",
                stream=["1", "2"],
                allow_missing=True
            )),
            html="fastp/html/pe/{sample}_{status}.fastp.html",
            json="fastp/json/pe/{sample}_{status}.fastp.json"
        message: "Cleaning {wildcards.status} {wildcards.sample} with Fastp"
        threads: 10
        resources:
            mem_mb=lambda wildcard, attempt: min(attempt * 4096, 15360),
            time_min=lambda wildcard, attempt: attempt * 45,
            tmpdir="tmp"
        params:
            adapters=config.get("fastp_adapters", None),
            extra=config.get("fastp_extra", "")
        log:
            "logs/fastp/{sample}.{status}.log"
        wrapper:
            "bio/fastp"


    #################################################
    ### Gather files from iRODS or mounting point ###
    #################################################

    rule bigr_copy:
        output:
            "reads/{status}/{sample}.{stream}.fq.gz"
        message:
            "Gathering {wildcards.status} {wildcards.sample} fastq files "
            "({wildcards.stream})"
        threads: 1
        resources:
            mem_mb=lambda wildcard, attempt: min(attempt * 1024, 2048),
            time_min=lambda wildcard, attempt: attempt * 45,
            tmpdir="tmp"
        params:
            input=lambda w, output: fastq_links[w.status][output[0]]
        log:
            "logs/bigr_copy/{status}/{sample}.{stream}.log"
        wrapper:
            "bio/BiGR/copy"


    ###########################
    ### Datasets indexation ###
    ###########################

    index_datasets_config = {
        "genome": config["ref"]["fasta"]
    }

    module index_datasets:
        snakefile: "../../meta/bio/index_datasets/test/Snakefile"
        config: index_datasets_config

    use rule samtools_faidx from index_datasets

    use rule picard_create_sequence_dictionnary from index_datasets

    rule sambamba_index_bam:
        input:
            "{tool}/{subcommand}/{sample}_{status}.bam"
        output:
            "{tool}/{subcommand}/{sample}_{status}.bam.bai"
        message:
            "Indexing {wildcards.sample} ({wildcards.status}) "
            "from {wildcards.tool}:{wildcards.subcommand}"
        threads: 8
        resources:
            mem_mb=lambda wildcards, attempt: attempt * 1024 * 16,
            time_min=lambda wildcards, attempt: attempt * 35,
            tmpdir="tmp"
        log:
            "sambamba/index/{tool}_{subcommand}/{sample}_{status}.log"
        params:
            extra = ""
        wrapper:
            "bio/sambamba/index"




Authors
-------


* Thibault Dayris

* M boyba Diop

* Marc Deloger
