import datetime
import logging
import os
import pandas
import sys
from pathlib import Path

worflow_source_dir = Path(snakemake.workflow.srcdir("."))
common = str(worflow_source_dir / ".." / "common" / "python")
sys.path.append(common)

from file_manager import *
from files_linker import *
from write_yaml import *
from messages import *
from snakemake.utils import min_version
min_version("6.0")

logging.basicConfig(
    filename="snakemake.variant_calling_somatic.log",
    filemode="w",
    level=logging.DEBUG
)

container: "docker://continuumio/miniconda3:4.4.10"
localrules: bigr_copy

ruleorder: sambamba_index_bam > sambamba_index
ruleorder: gatk_filter_mutect_calls > tabix_index
ruleorder: mutect2_somatic > tabix_index
ruleorder: create_snpeff_snpfit_data_input_dir > tabix_index

default_config = read_yaml(worflow_source_dir / "config.hg38.yaml")
configfile: get_config(default_config)
design = get_design(os.getcwd(), search_fastq_somatic)
#design = design.head(2).tail(1)
design.dropna(inplace=True)
#print(design)

design.index = design["Sample_id"]
#design.drop(index="s070", inplace=True)

wildcard_constraints:
    sample = r"|".join(design["Sample_id"]),
    stream = r"1|2|R1|R2",
    status = r"normal|tumor",
    content = r"snp|indel"


fastq_links = link_fq_somatic(
    sample_names=design.Sample_id,
    n1_paths=design.Upstream_file_normal,
    t1_paths=design.Upstream_file_tumor,
    n2_paths=design.Downstream_file_normal,
    t2_paths=design.Downstream_file_tumor,
)

ruleorder: fix_annotation_for_gatk > pbgzip_compress
ruleorder: gatk_variant_filtration > pbgzip_compress

rule all:
    input:
        #maf="maf/complete.maf",
        #mafs=expand(
        #    "maf/maftools/{sample}.maf",
        #    sample=design["Sample_id"].tolist()
        #),
        #calls=expand(
        #   "maf/occurence_annotated/{sample}.vcf.gz{index}",
        #   sample=design["Sample_id"].tolist(),
        #   index=["", ".tbi"]
        # ),
        #mutect2=expand(
        #    "bcftools/mutect2/{sample}.vcf.gz",
        #    sample=design["Sample_id"].tolist()
        #),
        #mutect2_tbi=expand(
        #    "bcftools/mutect2/{sample}.vcf.gz.tbi",
        #    sample=design["Sample_id"].tolist()
        #),
        #annotated_vcf=expand(
        #    "snpeff_snpsift/results_to_upload",
        #),
        facets=expand(
            "facets/{sample}/{sample}.{ext}",
            sample=design["Sample_id"].tolist(),
            ext=["vcf.gz", "cnv.png", "cov.pdf", "spider.pdf", "csv.gz"]
        ),
        calls=expand(
            "snpeff_snpsift/snpsift/fixed/{sample}.vcf.gz",
            sample=design["Sample_id"]
        ),
        calls_index=expand(
            "snpeff_snpsift/snpsift/fixed/{sample}.vcf.gz.tbi",
            sample=design["Sample_id"]
        ),
        table=expand(
            "snpeff_snpsift/snpsift/extractFields/{sample}.tsv",
            sample=design["Sample_id"]
        ),
        html="snpeff_snpsift/multiqc/SnpEff_annotation.html",
        html_data="snpeff_snpsift/multiqc/SnpEff_annotation_data",
        snpeff=expand(
            "snpeff_snpsift/snpeff/csvstats/{sample}.{ext}",
            sample=design["Sample_id"],
            ext=["csv", "genes.txt"]
        ),
        #varscan2=expand(
        #    "bcftools/varscan2/{sample}.vcf.gz",
        #    sample=design["Sample_id"].tolist()
        #),
        #varscan2_tbi=expand(
        #    "bcftools/varscan2/{sample}.vcf.gz.tbi",
        #    sample=design["Sample_id"].tolist()
        #),
        msisensor=expand(
            "msisensor/{sample}/{sample}.msi",
            sample=design["Sample_id"].tolist()
        ),
        qc="multiqc/variant_calling_somatic.html",
        #calling_result="final.vcf.list"
        results=expand("pandas/filter/{how}/dp{dp}/{sample}.dp{dp}.tsv",
            how=["all", "census_only", "oncokb_only"] if config.get("ANMO", False) is True else ["all"],
            sample=design["Sample_id"].tolist(),
            dp=["10", "40", "60"]
        ),
        results_xl=expand("pandas/filter/{how}/dp{dp}/{sample}.dp{dp}.xlsx",
            how=["all", "census_only", "oncokb_only"] if config.get("ANMO", False) is True else ["all"],
            sample=design["Sample_id"].tolist(),
            dp=["10", "40", "60"]
        ),
        tmb="TMB.tsv"
    output:
        directory("results_to_upload")
    message:
        "Finishing the WES Somatic Variant Calling"
    threads: 1
    resources:
        mem_mb=lambda wildcards, attempt: attempt * 768,
        time_min=lambda wildcards, attempt: attempt * 45,
        tmpdir="tmp"
    log:
        "logs/resultdir.log"
    params:
        mk="--parents --verbose",
        rs="--checksum --verbose --human-readable",
        dirlist=" ".join([f"results_to_upload/{content}" for content in ["msi", "tmb", "vcf", "tsv", "xlsx", "cnv", "qc"]])
    shell:
        "mkdir {params.mk} {params.dirlist} > {log} 2>&1 && "
        "rsync {params.rs} {input.msisensor} {output}/msi/ >> {log} 2>&1 && "
        "rsync {params.rs} {input.results} {output}/tsv/ >> {log} 2>&1 && "
        "rsync {params.rs} {input.results} {output}/xlsx/ >> {log} 2>&1 && "
        "rsync {params.rs} {input.facets} {output}/cnv/ >> {log} 2>&1 && "
        "rsync {params.rs} {input.calls} {input.calls_index} {output}/vcf/ >> {log} 2>&1 && "
        "rsync {params.rs} {input.html} {input.qc} {output}/qc/ >> {log} 2>&1 "


#########################
### ANMO-like filters ###
#########################


rule pandas_filter_tsv:
   input:
      table="snpeff_snpsift/snpsift/extractFields/{sample}.tsv"
   output:
      table="pandas/filter/{how}/dp{dp}/{sample}.dp{dp}.tsv",
      xlsx="pandas/filter/{how}/dp{dp}/{sample}.dp{dp}.xlsx"
   threads: 1
   resources:
      mem_mb=lambda wildcards, attempt: attempt * 1024 * 8,
      time_min=lambda wildcards, attempt: attempt * 15,
      tmpdir="tmp"
   log:
      "logs/pandas_filter/{sample}/{how}.{dp}.log"
   params:
      new_cols = lambda wildcards: [
         ["Mutect2_Allele_Frequency", "=", f"{wildcards.sample}_tumor_AF"]
      ],
      prefixes = [
         ["Chromosome", "chr"]
      ],
      keep_column = lambda wildcards: [
            "Chromosome",
            "Start_Position",
            "Variant_ID",
            "SYMBOL",
            "Reference_Allele",
            "Tumor_Seq_Allele1",
            "Mutect2_Read_depth",
            "Tumor_Seq_Allele2",
            "HGVSc",
            "HGVSp",
            "VarOcc",
            "BIOTYPE",
            "IMPACT",
            "Variant_Classification",
            "Mutect2_Read_depth",
            "Mutect2_Allele_Frequency",
            "Variant_Type",
            "Tumor_Sample_Barcode",
            "dbSNP_Pubmed",
            "dbSNP_Clinical_Diagnostic_Assay",
            "dbNSFP_ExAC_AlleleFrequency",
            "Kaviar_Allele_Frequency",
            "MSigDb_Pathways",
            "dbNSFP_Polyphen2_HDIV_pred",
            "dbNSFP_SIFT_pred",
            "dbNSFP_FATHMM_pred",
            "dbNSFP_ClinPred_score",
            "dbNSFP_MutationTaster_pred",
            'dbNSFP_MutationAssessor_pred',
            "dbNSFP_MetaLR_pred",
            "dbNSFP_LIST_S2_pred",
            "dbNSFP_LRT_pred",
            "dbNSFP_BayesDel_noAF_pred",
            "dbNSFP_Aloft_pred",
            "Transcript_ID",
            "Gene",
            "Feature",
            "Filter",
            "Center",
            "Hugo_Symbol",
            "End_Position",
            "CancerGeneCensus_Gene_Symbol",
            "CancerGeneCensus_Name",
            "CancerGeneCensus_Entrez_GeneId",
            "CancerGeneCensus_Tier",
            "CancerGeneCensus_Somatic",
            "CancerGeneCensus_Germline",
            "CancerGeneCensus_Tumour_TypesSomatic",
            "OncoKB_Hugo_Symbol",
            "OncoKB_Entrez_Gene_ID",
            "OncoKB_GRCh37_Isoform",
            "OncoKB_GRCh37_RefSeq",
            "OncoKB_GRCh38_RefSeq",
            "OncoKB_OncoKB_Annotated",
            "OncoKB_Is_Oncogene",
            "OncoKB_Is_Tumor_Suppressor_Gene",
            "OncoKB_MSK_IMPACT",
            "OncoKB_MSK_HEME",
            f"{wildcards.sample}_normal_Reference_Allele",
            f"{wildcards.sample}_normal_Seq_Allele1",
            f"{wildcards.sample}_normal_Seq_Allele2",
            f"{wildcards.sample}_normal_DP",
            f"{wildcards.sample}_normal_AD_allele2",
            f"{wildcards.sample}_normal_AF",
            f"{wildcards.sample}_tumor_Reference_Allele",
            f"{wildcards.sample}_tumor_Seq_Allele1",
            f"{wildcards.sample}_tumor_Seq_Allele2",
            f"{wildcards.sample}_tumor_DP",
            f"{wildcards.sample}_tumor_AD_allele1",
            f"{wildcards.sample}_tumor_AD_allele2",
            f"{wildcards.sample}_tumor_AF",
      ],
      convert_cols_type = lambda wildcards: {
            "Mutect2_Allele_Frequency": "float",
            "Mutect2_Read_depth": "int",
            f"{wildcards.sample}_tumor_AF": "float",
            f"{wildcards.sample}_normal_AF": "float",
            #f"{wildcards.sample}_normal_AD_allele2": "float",
            f"{wildcards.sample}_tumor_DP": "int",
            #f"{wildcards.sample}_tumor_AD_allele2": "float",
      },
      filters = lambda wildcards: [
            # ["Variant_Classification", "!=", "downstream_gene_variant"],
            # ["Variant_Classification", "!=", "intergenic_region"],
            # ["Variant_Classification", "!=", "synonymous_variant"],
            # ["Variant_Classification", "!=", "non_coding_transcript_exon_variant"],
            # ["Variant_Classification", "!=", "upstream_gene_variant"],
            # ["Variant_Classification", "!=", "splice_region_variant&synonymous_variant"],
            # ["Variant_Classification", "!=", "non_coding_transcript_variant"],
            # ["Variant_Classification", "!=", "intron_variant"],
            ["Variant_Classification", "!=", "Synonymous_Variant"],
            #["Mutect2_Read_depth", ">=", int(wildcards.filter)],
            #['Mutect2_Allele_Frequency', ">=", 0.1],
            [f"{wildcards.sample}_tumor_AF", ">=", 0.1],
            [f"{wildcards.sample}_normal_AF", "<=", 0.1],
            #[f"{wildcards.sample}_normal_AD_allele2", "<=", float(wildcards.dp)],
            #[f"{wildcards.sample}_tumor_AD_allele2", ">=", float(wildcards.dp)],
            [f"{wildcards.sample}_tumor_DP", ">=", float(wildcards.dp)],
            #["dbNSFP_ExAC_AlleleFrequency", "<=", 0.05],
            ["VarOcc", "<=", len(design["Sample_id"].tolist()) - 1]
      ] if config.get("ANMO", False) is True else [],
      not_contains = lambda wildcards: [
            ["Filter", "germline"],
            ["Filter", "multiallelic"],
            #"Filter": "fragment",
            #"Filter": "contamination",
            #"Filter": "weak_evidence",
            #"Filter": "slippage",
            #"Filter": "strand_bias",
            #"Filter": "map_qual",
            ["Filter", "haplotype"],
            #"Filter": "base_qual",
            #["Filter", "DPBelow5"],
            ["Filter", "AboveFisherStrandBias"],
            ["Filter", "AboveStrandOddsRatio"],
            ["Filter", "BelowMQRankSum"],
            ["Filter", "BelowReadPosRankSum"],
            #["Filter", "VarOccAbove105"],
            #["Filter", f"DepthBelow{wildcards.filter}X"],
            #["Filter", "AFBelow40pct"]
      ],
      contains = lambda wildcards: [
            ["Filter", (
               "ExistsInCancerGeneCensus" if wildcards.how == "census_only" else (
                  "ExistsInOncoKB" if wildcards.how == "oncokb_only" else ""))]
      ],
      drop_duplicated_lines=True
   wrapper:
        "bio/pandas/filter_table"


##################
### VCF to MAF ###
##################

vcf_post_process_config = {
    "ncbi_build": config["params"].get("ncbi_build", "GRCh38"),
    "center": config["params"].get("center", "GustaveRoussy"),
    "annotation_tag": "ANN=",
    "sample_list": design["Sample_id"].to_list(),
    "genome": config["ref"]["fasta"],
    "known": config["ref"]["dbsnp"],
    "chr": config["params"]["chr"]
}

module vcf_post_process:
    snakefile: "../../meta/bio/vcf_post_process/test/Snakefile"
    config: vcf_post_process_config

use rule * from vcf_post_process


#################
### Gather QC ###
#################

rule multiqc:
    input:
        html=expand(
            "fastp/html/pe/{sample}_{status}.fastp.html",
            sample=design["Sample_id"],
            status=["normal", "tumor"]
        ),
        json=expand(
            "fastp/json/pe/{sample}_{status}.fastp.json",
            sample=design["Sample_id"],
            status=["normal", "tumor"]
        ),
        sambamba_metrics=expand(
            "sambamba/markdup/{sample}_{status}.bam",
            sample=design["Sample_id"],
            status=["normal", "tumor"]
        ),
        fastq_screen=expand(
            "fastq_screen/{sample}.{stream}.{status}.fastq_screen.{ext}",
            sample=design["Sample_id"],
            stream=["1", "2"],
            ext=["txt", "png"],
            status=["normal", "tumor"]
        ),
        picard_summary=expand(
            "picard/alignment_summary/{sample}_{status}.summary.txt",
            sample=design["Sample_id"],
            status=["normal", "tumor"]
        ),
        snpeff=expand(
            "snpeff_snpsift/snpeff/csvstats/{sample}.genes.txt",
            sample=design["Sample_id"]
        )
    output:
        report(
            "multiqc/variant_calling_somatic.html",
            caption="../common/reports/multiqc.rst",
            category="Quality Controls"
        )
    message:
        "Aggregating quality reports from SnpEff"
    threads: 1
    resources:
        mem_mb=lambda wildcards, attempt: min(attempt * 1536, 10240),
        time_min=lambda wildcards, attempt: attempt * 35,
        tmpdir="tmp"
    log:
        "logs/multiqc.log"
    wrapper:
        "bio/multiqc"


rule alignment_summary:
    input:
        bam="sambamba/sort/{sample}_{status}.bam",
        bam_index=get_bai("sambamba/sort/{sample}_{status}.bam"),
        ref=config['ref']['fasta'],
        ref_idx=get_fai(config['ref']['fasta']),
        ref_dict=get_dict(config['ref']['fasta']),
    output:
        temp("picard/alignment_summary/{sample}_{status}.summary.txt")
    message:
        "Collecting alignment metrics on GATK recalibrated {wildcards.sample}"
        " (considering {wildcards.status})"
    threads: 1
    resources:
        mem_mb=lambda wildcards, attempt: attempt * 1020,
        time_min=lambda wildcards, attempt: attempt * 45,
        tmpdir="tmp"
    log:
        "logs/picard/alignment_summary/{sample}_{status}.log"
    params:
        "VALIDATION_STRINGENCY=LENIENT "
        "METRIC_ACCUMULATION_LEVEL=null "
        "METRIC_ACCUMULATION_LEVEL=SAMPLE"
    wrapper:
        "bio/picard/collectalignmentsummarymetrics"


rule fastq_screen:
    input:
        "reads/{status}/{sample}.{stream}.fq.gz"
    output:
        txt=temp("fastq_screen/{sample}.{stream}.{status}.fastq_screen.txt"),
        png=temp("fastq_screen/{sample}.{stream}.{status}.fastq_screen.png")
    message:
        "Assessing quality of {wildcards.sample}, {wildcards.stream}"
        " (considering {wildcards.status})"
    threads: config.get("threads", 20)
    resources:
        mem_mb=lambda wildcard, attempt: min(attempt * 1024 * 8, 20480),
        time_min=lambda wildcard, attempt: attempt * 75,
        tmpdir="tmp"
    params:
        fastq_screen_config=config["fastq_screen"],
        subset=100000,
        aligner='bowtie2'
    log:
        "logs/fastqc/{sample}.{stream}.{status}.log"
    wrapper:
        "bio/fastq_screen"


######################
### MSI sensor pro ###
######################

msi_sensor_config = {
    "fasta": config["ref"]["fasta"],
    "bed": config["ref"]["capture_kit_bed"],
    "msi_scan_extra": config["msisensor_pro"].get("scan", ""),
    "msi_pro_extra": config["msisensor_pro"].get("msi", "")
}

module missensor_pro_meta:
    snakefile: "../../meta/bio/msi_sensor_pro/test/Snakefile"
    config: msi_sensor_config

use rule * from missensor_pro_meta


###########
### TMB ###
###########

somatic_tmb_config = {
    "min_coverage": config["tmb"].get("min_coverage"),
    "tmb_highness_threshold": config["tmb"].get("tmb_highness_threshold", 10),
    "allele_depth_keyname": config["tmb"].get("allele_depth_keyname", "AD"),
    "bed": config["ref"]["capture_kit_bed"],
    "sample_list": design["Sample_id"]
}


module somatic_tmb:
    snakefile: "../../meta/bio/somatic_tmb/test/Snakefile"
    config: somatic_tmb_config


use rule * from somatic_tmb


use rule extract_somatic_mutations from somatic_tmb with:
    input:
        vcf = "snpeff_snpsift/snpsift/fixed/{sample}.vcf.gz"

##################
### CNV Facets ###
##################


rule add_chr_to_pileup:
    input:
        "samtools/mpileup/{sample}.mpileup.gz"
    output:
        temp("samtools/mpileup/{sample}.chr.mpileup.gz")
    message:
        "Adding 'chr' to sequences on {wildcards.sample}"
    threads: 3
    resources:
        mem_mb=lambda wildcards, attempt: attempt * 512,
        time_min=lambda wildcards, attempt: attempt * 75,
        tmpdir="tmp"
    log:
        "logs/cnv_facets/add_chr/{sample}.log"
    params:
        gzip = "-c",
        awk = 'BEGIN {FS="\\t"; print "Chromosome\\tPosition\\tRef\\tAlt\\tFile1R\\tFile1A\\tFile1E\\tFile1D\\tFile2R\\tFile2A\\tFile2E\\tFile2D"} {if ($1 !~ /^chr*/) {print "chr"$0} else {print $0}}'
    shell:
        "gunzip {params.gzip} {input} | "
        "awk '{params.awk}' | "
        "gzip {params.gzip} > {output} 2> {log}"


rule cnv_facets:
    input:
        #pileup="samtools/mpileup/{sample}.chr.mpileup.gz",
        tumor_bam="sambamba/markdup/{sample}_tumor.bam",
        normal_bam="sambamba/markdup/{sample}_normal.bam",
        vcf=config["ref"]["dbsnp"],
        vcf_index=get_tbi(config["ref"]["dbsnp"]),
        bed=config["ref"]["capture_kit_bed"]
    output:
        vcf="facets/{sample}/{sample}.vcf.gz",
        profile="facets/{sample}/{sample}.cnv.png",
        coverage="facets/{sample}/{sample}.cov.pdf",
        spider="facets/{sample}/{sample}.spider.pdf",
        pileup="facets/{sample}/{sample}.csv.gz"
    message:
        "Searching for CNV in {wildcards.sample} with Facets"
    threads: 10
    resources:
        mem_mb=lambda wildcards, attempt: (attempt * 1024 * 20) + 102400,
        time_min=lambda wildcards, attempt: attempt * 60 * 2,
        tmpdir="tmp"
    params:
        extra=config.get(
            "facets_extra",
            "--snp-count-orphans --gbuild hg38 --nbhd-snp 250"
        ),
        prefix="facets/{sample}/{sample}"
    log:
        "logs/facets/cnv/{sample}.log"
    wrapper:
        "bio/facets/cnv"


#################################
### FINAL VCF FILE INDEXATION ###
#################################

module compress_index_vcf_meta:
    snakefile: "../../meta/bio/compress_index_vcf/test/Snakefile"
    config: config

use rule * from compress_index_vcf_meta


use rule tabix_index from compress_index_vcf_meta as snp_indel_tabix_index with:
    input:
        "{tool}/{subcommand}/{sample}.{content}.vcf.gz"
    output:
        "{tool}/{subcommand}/{sample}.{content}.vcf.gz.tbi"
    message:
        "Indexing {wildcards.sample} (from somatic varscan "
        "{wildcards.content}) with tabix."
    log:
        "logs/{tool}/{subcommand}/tabix/index/{sample}.{content}.log"


use rule pbgzip_compress from compress_index_vcf_meta as si_pbgzip with:
    input:
        "{tool}/{subcommand}/{sample}.{content}.vcf"
    output:
        "{tool}/{subcommand}/{sample}.{content}.vcf.gz"
    message:
        "Compressnig {wildcards.sample} (from somatic varscan "
        "{wildcards.content}) with pbgzip."
    log:
        "logs/{tool}/{subcommand}/pgbzip/varcsanc2/{sample}.{content}.log"


######################
### VCF annotation ###
######################

rule annotate_vcf:
    input:
        design="design.tsv",
        config="snpeff_snpsift/config.yaml",
        calls=expand(
            "snpeff_snpsift/data_input/{sample}.vcf.gz",
            sample=design["Sample_id"]
        ),
        calls_index=expand(
            get_tbi("snpeff_snpsift/data_input/{sample}.vcf.gz"),
            sample=design["Sample_id"]
        ),
    output:
        calls=temp(expand(
            "snpeff_snpsift/results_to_upload/VCF/{sample}.vcf.gz",
            sample=design["Sample_id"]
        )),
        calls_index=temp(expand(
            "snpeff_snpsift/results_to_upload/VCF/{sample}.vcf.gz.tbi",
            sample=design["Sample_id"]
        )),
        table=temp(expand(
            "snpeff_snpsift/results_to_upload/TSV/{sample}.tsv",
            sample=design["Sample_id"]
        )),
        html="snpeff_snpsift/results_to_upload/QC/SnpEff_annotation.html",
        raw_html=temp("snpeff_snpsift/multiqc/SnpEff_annotation.html"),
        raw_html_data=temp(directory("snpeff_snpsift/multiqc/SnpEff_annotation_data")),
        bigr_format_to_info=temp(expand(
            "snpeff_snpsift/bigr/format_to_info/{sample}.vcf",
            sample=design["Sample_id"]
        )),
        bigr_occurence_per_chr=temp(expand(
            "snpeff_snpsift/bigr/occurence/{chrom}.txt",
            chrom=config["params"]["chr"]
        )),
        bigr_occurence_annot=temp(expand(
            "snpeff_snpsift/bigr/occurence_annotated/{sample}.vcf",
            sample=design["Sample_id"]
        )),
        bigr_occurences=temp("snpeff_snpsift/bigr/occurences/all_chrom.txt"),
        config=temp("snpeff_snpsift/config.yaml"),
        column_description=temp("snpeff_snpsift/columns_description.txt"),
        design=temp("snpeff_snpsift/design.tsv"),
        logs=temp(directory("logs")),
        db_descriptions=temp(expand(
            "snpeff_snpsift/{db}/description.txt",
            db=["revel", "mane", "mistic"]
        )),
        snpsift_intermediar=temp(expand(
            "snpeff_snpsift/snpsift/{db}/{sample}.vcf",
            db=["clinvar", "dbnsfp", "dbsnp", "gmt", "kaviar", "vartype"],
            sample=design["Sample_id"]
        )),
        splice_ai=temp(expand(
            "snpeff_snpsift/splice_ai/annot/{sample}.vcf.gz",
            sample=design["Sample_id"]
        )),
        tmpdir=temp(directory("snpeff_snpsift/tmp")),
        vcftools=temp(expand(
            "snpeff_snpsift/vcftools/{db}/{sample}.vcf.gz",
            db=["revel", "mane", "mistic"],
            sample=design["Sample_id"]
        )),
        snpeff_raw_call=temp(expand(
            "snpeff_snpsift/snpeff/calls/{sample}.vcf{ext}",
            sample=design["Sample_id"],
            ext=["", ".gz", ".gz.tbi"]
        )),
        snpeff_raw_html=temp(expand(
            "snpeff_snpsift/snpeff/html/{sample}.html",
            sample=design["Sample_id"],
        )),
        snpeff_raw_csvstats=temp(expand(
            "snpeff_snpsift/snpeff/csvstats/{sample}.{ext}",
            sample=design["Sample_id"],
            ext=["csv", "genes.txt"]
        )),
        snpsift_raw_fields=temp(expand(
            "snpeff_snpsift/snpsift/extractFields/{sample}.tsv",
            sample=design["Sample_id"]
        )),
        snpsift_raw_vcf=temp(expand(
            "snpeff_snpsift/snpsift/fixed/{sample}.{ext}",
            sample=design["Sample_id"],
            ext=["vcf", "vcf.gz", "vcf.gz.tbi"]
        )),
        #results=directory("snpeff_snpsift/results_to_upload")
    message:
        "Annotating VCF"
    threads: 2
    resources:
        mem_mb=lambda wildcards, attempt: attempt * 1024 * 5,
        time_min=lambda wildcards, attempt: attempt * 60 * 4,
        tmpdir="tmp"
    handover: True
    log:
        "logs/snpeff_snpsift_pipeline.log"
    params:
        outdir="snpeff_snpsift",
        pipeline_path=config.get(
            "snpeff_snpsift_run_path",
            "/mnt/beegfs/pipelines/snakemake-wrappers/bigr_pipelines/snpeff_snpsift/run.sh"
        ),
        smk="--nt",
        organism = config["params"].get("organism", "hg38")
    shell:
        "cd {params.outdir} && "
        "bash {params.pipeline_path} {params.organism} {params.smk} | tee -a ${{OLDPWD}}/{log} 2>&1"



rule create_snpeff_snpfit_data_input_dir:
    input:
        vcf="mutect2/corrected/{sample}.vcf.gz",
        tbi=get_tbi("mutect2/corrected/{sample}.vcf.gz"),
    output:
        vcf=temp("snpeff_snpsift/data_input/{sample}.vcf.gz"),
        tbi=temp("snpeff_snpsift/data_input/{sample}.vcf.gz.tbi"),
    threads: 1
    resources:
        mem_mb=1024,
        time_min=5,
        tmpdir="tmp"
    log:
        "logs/link/snpeff_snpsift/{sample}.log"
    params:
        "--symbolic --force --relative --verbose"
    shell:
        "ln {params} {input.vcf} {output.vcf} > {log} 2>&1 && "
        "ln {params} {input.tbi} {output.tbi} >> {log} 2>&1 "


rule create_snpeff_snpfit_config:
    input:
        "config.yaml"
    output:
        temp("snpeff_snpsift/config.yaml")
    threads: 1
    resources:
        mem_mb=1024,
        time_min=5,
        tmpdir="tmp"
    log:
        "logs/link/snpeff_snpsift/config.log"
    params:
        "--symbolic --force --relative --verbose"
    shell:
        "ln {params} {input} {output} > {log} 2>&1 && "


#####################################
### Merge variant calling results ###
#####################################

# module metacaller_somatic_meta:
#     snakefile: "../../meta/bio/meta_caller_somatic/test/Snakefile"
#     config: {"genome": config["ref"]["fasta"], "bed": config["ref"]["capture_kit_bed"]}
#
#
# use rule * from metacaller_somatic_meta as *


############################################################################
### Correcting Mutect2 :                                                 ###
### AS_FilterStatus: Number=1 and not Number=A which violates VCF format ###
############################################################################

rule correct_mutect2_vcf:
    input:
        "bcftools/mutect2/{sample}.vcf.gz"
    output:
        temp("mutect2/corrected/{sample}.vcf")
    message:
        "Fixing AS_FilterStrand format error"
        " on {wildcards.sample}"
    threads: 2
    resources:
        mem_mb=lambda wildcards, attempt: attempt * 256,
        time_min=lambda wildcards, attempt: attempt * 20,
        tmpdir="tmp"
    log:
        "logs/mutect2/correct_fields/{sample}.log"
    params:
        fix_as_filterstatus="'s/ID=AS_FilterStatus,Number=A/ID=AS_FilterStatus,Number=1/g'"
    shell:
        "(gunzip -c {input} | "
        "sed {params.fix_as_filterstatus}) "
        "> {output} 2> {log}"

###############################
### Variant calling Mutect2 ###
###############################

gatk_mutect2_somatic_config = {
    "genome": config["ref"]["fasta"],
    "known": config["ref"]["af_only"],
    "bed": config["ref"]["capture_kit_bed"],
    "dbsnp": config["ref"]["dbsnp"],
    "sample_list": design["Sample_id"].to_list(),
    "chrom": config["params"]["chr"]
}


module gatk_mutect2_somatic_meta:
    snakefile: "../../meta/bio/mutect2_somatic/test/Snakefile"
    config: gatk_mutect2_somatic_config


use rule * from gatk_mutect2_somatic_meta


################################
### Variant Calling Varscan2 ###
################################

varscan2_somatic_config = {
    "genome": config["ref"]["fasta"],
    "bed": config["ref"]["capture_kit_bed"]
}

module varscan2_somatic_meta:
    snakefile: "../../meta/bio/varscan2_somatic/test/Snakefile"
    config: varscan2_somatic_config

use rule * from varscan2_somatic_meta


##############################
### GATK BAM RECALIBRATION ###
##############################

gatk_bqsr_config = {
    "threads": config["threads"],
    "genome": config["ref"]["fasta"],
    "dbsnp": config["ref"]["dbsnp"],
    "base_recal_extra": "",
    "apply_base_recal_extra": config.get(
        "gatk", {"apply_base_recal_extra": "--create-output-bam-index"}
    ).get("apply_base_recal_extra", "--create-output-bam-index")
}

module gatk_bqsr_meta:
    snakefile: "../../meta/bio/gatk_bqsr/test/Snakefile"
    config: gatk_bqsr_config


use rule gatk_apply_baserecalibrator from gatk_bqsr_meta with:
    input:
        bam="sambamba/markdup/{sample}_{status}.bam",
        bam_index=get_bai("sambamba/markdup/{sample}_{status}.bam"),
        ref=config['ref']['fasta'],
        ref_idx=get_fai(config['ref']['fasta']),
        ref_dict=get_dict(config['ref']['fasta']),
        recal_table="gatk/recal_data_table/{sample}_{status}.grp"
    output:
        bam="gatk/recal_bam/{sample}_{status}.bam",
        bai=get_bai("gatk/recal_bam/{sample}_{status}.bam")
    message:
        "Applying BQSR on {wildcards.status} {wildcards.sample} with GATK"
    params:
        extra=config.get(
            "gatk", {"apply_base_recal_extra", "--create-output-bam-index"}
        ).get("apply_base_recal_extra", "--create-output-bam-index")
    log:
        "logs/gatk/applybqsr/{sample}.{status}.log"


use rule gatk_compute_baserecalibration_table from gatk_bqsr_meta with:
    input:
        bam="sambamba/markdup/{sample}_{status}.bam",
        bam_index=get_bai("sambamba/markdup/{sample}_{status}.bam"),
        ref=config['ref']['fasta'],
        ref_idx=get_fai(config['ref']['fasta']),
        ref_dict=get_dict(config['ref']['fasta']),
        known=config['ref']['dbsnp'],
        known_idx=get_tbi(config['ref']['dbsnp'])
    output:
        recal_table=temp("gatk/recal_data_table/{sample}_{status}.grp")
    message:
        "Compute BQSR table from {wildcards.status} {wildcards.sample} "
        "with GATK"
    log:
        "logs/gatk3/compute_bqsr/{sample}.{status}.log"


#####################
### Deduplicating ###
#####################

rule sambamba_markduplicates:
    input:
        bam="samtools/filter/{sample}_{status}.bam",
        bai=get_bai("samtools/filter/{sample}_{status}.bam")
    output:
        bam=temp("sambamba/markdup/{sample}_{status}.bam")
    message:
        "Removing duplicates on {wildcards.sample} ({wildcards.status})"
    threads: 10
    resources:
        mem_mb=lambda wildcards, attempt: attempt * 10240,
        time_min=lambda wildcards, attempt: attempt * 45,
        tmpdir="tmp"
    log:
        "logs/sambamba/markduplicates/{sample}_{status}.log"
    params:
        extra = config.get(
            "sambamba", {"markdup": "--remove-duplicates"}
        ).get("markdup", "--remove-duplicates")
    wrapper:
        "bio/sambamba/markdup"



"""
Filter a bam over the capturekit bed file
"""
rule samtools_filter_bed:
    input:
        "sambamba/sort/{sample}_{status}.bam",
        fasta=config["ref"]["fasta"],
        fasta_idx=get_fai(config["ref"]["fasta"]),
        fasta_dict=get_dict(config["ref"]["fasta"]),
        bed=config["ref"]["capture_kit_bed"]
    output:
        temp("samtools/filter/{sample}_{status}.bam")
    threads: 10
    resources:
        mem_mb=lambda wildcards, attempt: attempt * 2048,
        time_min=lambda wildcards, attempt: attempt * 15,
        tmpdir="tmp"
    params:
        extra="-h -h"
    log:
        "logs/samtools/filter/{sample}_{status}.log"
    wrapper:
        "bio/samtools/view"


###################
### BWA MAPPING ###
###################

module bwa_fixmate_meta:
    snakefile: "../../meta/bio/bwa_fixmate/test/Snakefile"
    config: {"threads": config["threads"], "genome": config["ref"]["fasta"]}


def get_best_bwa_index():
    """Return cached data if available"""
    if config["ref"]["fasta"] == "/mnt/beegfs/database/bioinfo/Index_DB/Fasta/Ensembl/GRCh38.99/GRCh38.99.homo_sapiens.dna.main_chr.fasta":
        return multiext(
            "bwa_mem2/index/genome.hg38", ".0123", ".amb", ".ann", ".pac"
        )
    elif config["ref"]["fasta"] == "/mnt/beegfs/database/bioinfo/Index_DB/Fasta/Ensembl/GRCh37.75/Homo_sapiens.GRCh37.75.dna.primary_assembly.fa":
        return multiext(
            "bwa_mem2/index/genome.hg19", ".0123", ".amb", ".ann", ".pac"
        )
    elif config["ref"]["fasta"] == "/mnt/beegfs/database/bioinfo/Index_DB/Fasta/Ensembl/GRCm38.99/GRCm38.99.mus_musculus.dna.fasta":
        return multiext(
            "bwa_mem2/index/genome.mm10", ".0123", ".amb", ".ann", ".pac"
        )
    return multiext(
        "bwa_mem2/index/genome", ".0123", ".amb", ".ann", ".pac"
    )


use rule sambamba_index from bwa_fixmate_meta with:
    input:
        "sambamba/sort/{sample}_{status}.bam"
    output:
        temp("sambamba/sort/{sample}_{status}.bam.bai")
    message:
        "Indexing mapped reads of {wildcards.status} {wildcards.sample}"
    log:
        "logs/sambamba/sort/{sample}.{status}.log"


use rule sambamba_sort_coordinate from bwa_fixmate_meta with:
    input:
        mapping="samtools/fixmate/{sample}_{status}.bam"
    output:
        mapping=temp("sambamba/sort/{sample}_{status}.bam")
    message:
        "Sorting {wildcards.status} {wildcards.sample} reads by position"
    log:
        "logs/sambamba/sort/{sample}.{status}.log"


use rule samtools_fixmate from bwa_fixmate_meta with:
    input:
        "bwa_mem2/mem/{sample}_{status}.bam"
    output:
        temp("samtools/fixmate/{sample}_{status}.bam")
    message:
        "Fixing mate annotation on {wildcards.status} "
        "{wildcards.sample} with Samtools"
    log:
        "logs/samtools/fixmate/{sample}.{status}.log"


use rule bwa_mem from bwa_fixmate_meta with:
    input:
        reads=expand(
            "fastp/trimmed/pe/{sample}_{status}.{stream}.fastq",
            stream=["1", "2"],
            allow_missing=True
        ),
        index=get_best_bwa_index()
    output:
        temp("bwa_mem2/mem/{sample}_{status}.bam")
    message:
        "Mapping {wildcards.status} {wildcards.sample} with BWA"
    params:
        index=lambda wildcards, input: os.path.splitext(input["index"][0])[0],
        extra="-R '@RG\tID:{sample}_{status}\tSM:{sample}_{status}\tPU:{sample}_{status}\tPL:ILLUMINA\tCN:IGR\tDS:WES\tPG:BWA-MEM2' -M -A 2 -E 1",
        sort="samtools",         # We chose Samtools to sort by queryname
        sort_order="queryname",  # Queryname sort is needed for a fixmate
        sort_extra="-m 1536M"     # We extand the sort buffer memory
    log:
        "logs/bwa_mem2/mem/{sample}.{status}.log"

# rule bwa_samblaster_sambamba:
#     input:
#         reads=expand(
#             "fastp/trimmed/pe/{sample}_{status}.{stream}.fastq",
#             stream=["1", "2"],
#             allow_missing=True
#         ),
#         index=get_best_bwa_index()
#     output:
#         bam=temp("bwa_mem2/mem/{sample}_{status}.bam")
#     message:
#         "Mapping {wildcards.status} {wildcards.sample} with BWA, removing "
#         "duplicates with Samblaster, sorting and filtering with Sambamba."
#     threads:
#         min(config.get("threads", 20), 20)
#     threads: 1
#     resources:
#         mem_mb=lambda wildcards, attempt: attempt * 6144 + 61440,
#         time_min=lambda wildcards, attempt: attempt * 120,
#         tmpdir="tmp"
#     shadow: "shallow"
#     params:
#         index=get_best_bwa_index()[0].split('.')[0],
#         extra=lambda wildcards: f"-R '@RG\tID:{wildcards.sample}_{wildcards.status}\tSM:{wildcards.sample}_{wildcards.status}\tPU:{wildcards.sample}_{wildcards.status}\tPL:ILLUMINA\tCN:IGR\tDS:WES\tPG:BWA-MEM2' -M -A 2 -E 1",
#         sort_extra=config.get("sambamba", {"sort_extra": ""}).get("sort_extra", ""),
#         samblaster_extra=config.get("samblaster", {"extra": ""}).get("extra", ""),
#         sambamba_view_extra=config.get("sambamba", {"view_extra": "-h"}).get("view_extra", "-h")
#     log:
#         "logs/bwa_samblaster_sambamba/{sample}_{status}.log"
#




use rule bwa_index from bwa_fixmate_meta with:
    input:
        config["ref"]["fasta"]


use rule bwa_index_hg38 from bwa_fixmate_meta

use rule bwa_index_hg19 from bwa_fixmate_meta

use rule bwa_index_mm10 from bwa_fixmate_meta


############################
### FASTP FASTQ CLEANING ###
############################

rule fastp_clean:
    input:
        sample=expand(
            "reads/{status}/{sample}.{stream}.fq.gz",
            stream=["1", "2"],
            allow_missing=True
        ),
    output:
        trimmed=temp(expand(
            "fastp/trimmed/pe/{sample}_{status}.{stream}.fastq",
            stream=["1", "2"],
            allow_missing=True
        )),
        html="fastp/html/pe/{sample}_{status}.fastp.html",
        json="fastp/json/pe/{sample}_{status}.fastp.json"
    message: "Cleaning {wildcards.status} {wildcards.sample} with Fastp"
    threads: 10
    resources:
        mem_mb=lambda wildcard, attempt: min(attempt * 4096, 15360),
        time_min=lambda wildcard, attempt: attempt * 45,
        tmpdir="tmp"
    params:
        adapters=config.get("fastp_adapters", None),
        extra=config.get("fastp_extra", "")
    log:
        "logs/fastp/{sample}.{status}.log"
    wrapper:
        "bio/fastp"


#################################################
### Gather files from iRODS or mounting point ###
#################################################

rule bigr_copy:
    output:
        "reads/{status}/{sample}.{stream}.fq.gz"
    message:
        "Gathering {wildcards.status} {wildcards.sample} fastq files "
        "({wildcards.stream})"
    threads: 1
    resources:
        mem_mb=lambda wildcard, attempt: min(attempt * 1024, 2048),
        time_min=lambda wildcard, attempt: attempt * 45,
        tmpdir="tmp"
    params:
        input=lambda w, output: fastq_links[w.status][output[0]]
    log:
        "logs/bigr_copy/{status}/{sample}.{stream}.log"
    wrapper:
        "bio/BiGR/copy"


###########################
### Datasets indexation ###
###########################

index_datasets_config = {
    "genome": config["ref"]["fasta"]
}

module index_datasets:
    snakefile: "../../meta/bio/index_datasets/test/Snakefile"
    config: index_datasets_config

use rule samtools_faidx from index_datasets

use rule picard_create_sequence_dictionnary from index_datasets

rule sambamba_index_bam:
    input:
        "{tool}/{subcommand}/{sample}_{status}.bam"
    output:
        "{tool}/{subcommand}/{sample}_{status}.bam.bai"
    message:
        "Indexing {wildcards.sample} ({wildcards.status}) "
        "from {wildcards.tool}:{wildcards.subcommand}"
    threads: 8
    resources:
        mem_mb=lambda wildcards, attempt: attempt * 1024 * 16,
        time_min=lambda wildcards, attempt: attempt * 35,
        tmpdir="tmp"
    log:
        "sambamba/index/{tool}_{subcommand}/{sample}_{status}.log"
    params:
        extra = ""
    wrapper:
        "bio/sambamba/index"
